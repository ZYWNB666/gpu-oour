# GPU åˆ©ç”¨ç‡ç›‘æ§åˆ†æç³»ç»Ÿ

åŸºäº Prometheus + FastAPI çš„ GPU åˆ©ç”¨ç‡æ™ºèƒ½ç›‘æ§åˆ†æç³»ç»Ÿï¼Œä¸“ä¸º Kubeflow GPU å‡ºå€Ÿåœºæ™¯è®¾è®¡ã€‚

## ğŸ“‹ åŠŸèƒ½ç‰¹æ€§

- âœ… **å¤šç»´åº¦è¯„åˆ†**ï¼šç»¼åˆ GPU åˆ©ç”¨ç‡ã€æ˜¾å­˜ä½¿ç”¨ã€å†…å­˜å¸¦å®½ã€åŠŸç‡ç­‰å¤šä¸ªæŒ‡æ ‡è®¡ç®—ç»¼åˆè¯„åˆ†
- âœ… **å¤šçº¿ç¨‹å¹¶å‘**ï¼šæ”¯æŒåŒæ—¶ç›‘æ§å¤§é‡ GPUï¼Œé«˜æ•ˆå¹¶å‘æŸ¥è¯¢
- âœ… **ç§Ÿæˆ·å…³è”**ï¼šè‡ªåŠ¨å…³è” Pod/Namespace ä¿¡æ¯ï¼Œæ˜ç¡®ç§Ÿæˆ·è´£ä»»
- âœ… **æ™ºèƒ½åˆ¤å®š**ï¼šåŒºåˆ† idle/low/normal/high å››ç§çŠ¶æ€
- âœ… **è‡ªåŠ¨è°ƒåº¦**ï¼šå®šæ—¶è½®è¯¢åˆ†æï¼Œæ”¯æŒæ‰‹åŠ¨è§¦å‘
- âœ… **RESTful API**ï¼šæä¾›å®Œæ•´çš„ API æ¥å£
- âœ… **å¯æ‰©å±•**ï¼šæ”¯æŒè°ƒç”¨å¤–éƒ¨æ§åˆ¶æ¥å£è¿›è¡Œè‡ªåŠ¨åŒ–æ“ä½œ

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Prometheus            â”‚
â”‚ (DCGM exporter GPU metrics)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       æ¯éš” N åˆ†é’Ÿæ‹‰å–
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU Monitor Service        â”‚
â”‚  - å¤šçº¿ç¨‹æ‹‰å–GPUæ•°æ®        â”‚
â”‚  - è®¡ç®—ç»¼åˆè¯„åˆ†             â”‚
â”‚  - å…³è”ç§Ÿæˆ·ä¿¡æ¯             â”‚
â”‚  - è§¦å‘æ§åˆ¶æ¥å£             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¤–éƒ¨æ§åˆ¶ç³»ç»Ÿ               â”‚
â”‚  (æˆæœ¬ä¼˜åŒ–/èµ„æºå›æ”¶)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š è¯„åˆ†ç®—æ³•

ç»¼åˆåˆ©ç”¨ç‡è¯„åˆ†é‡‡ç”¨åŠ æƒè®¡ç®—ï¼š

```
score = 0.5 Ã— GPUæ ¸å¿ƒåˆ©ç”¨ç‡
      + 0.2 Ã— å†…å­˜æ‹·è´åˆ©ç”¨ç‡
      + 0.2 Ã— æ˜¾å­˜ä½¿ç”¨ç‡
      + 0.1 Ã— åŠŸç‡ä½¿ç”¨ç‡
```

**çŠ¶æ€åˆ†ç±»**ï¼š
- `idle` (< 10%)ï¼šå®Œå…¨é—²ç½®
- `low` (10-20%)ï¼šä½åˆ©ç”¨ç‡
- `normal` (20-70%)ï¼šæ­£å¸¸ä½¿ç”¨
- `high` (> 70%)ï¼šé«˜è´Ÿè½½

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```powershell
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
.\venv\Scripts\Activate.ps1

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. é…ç½®æœåŠ¡

ç¼–è¾‘ `gpu_monitor.py` ä¸­çš„é…ç½®é¡¹ï¼Œæˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```python
class Config:
    PROM_URL = "http://your-prometheus:9090"
    THRESHOLD = 20  # ä½åˆ©ç”¨ç‡é˜ˆå€¼
    INTERVAL = 300  # è½®è¯¢å‘¨æœŸï¼ˆç§’ï¼‰
    MAX_WORKERS = 8  # å¹¶å‘çº¿ç¨‹æ•°
```

### 3. å¯åŠ¨æœåŠ¡

```powershell
# ç›´æ¥è¿è¡Œ
python gpu_monitor.py

# æˆ–ä½¿ç”¨ uvicorn
uvicorn gpu_monitor:app --host 0.0.0.0 --port 8080
```

### 4. è®¿é—®æœåŠ¡

- **æœåŠ¡ä¿¡æ¯**ï¼šhttp://localhost:8080/
- **å¥åº·æ£€æŸ¥**ï¼šhttp://localhost:8080/health
- **æ‰‹åŠ¨è§¦å‘åˆ†æ**ï¼šhttp://localhost:8080/analyze
- **æŸ¥çœ‹æœ€æ–°ç»“æœ**ï¼šhttp://localhost:8080/results
- **ä½åˆ©ç”¨ç‡ GPU**ï¼šhttp://localhost:8080/results/low
- **ç»Ÿè®¡ä¿¡æ¯**ï¼šhttp://localhost:8080/results/stats

## ğŸ”§ API æ–‡æ¡£

### 1. è·å–æœåŠ¡ä¿¡æ¯

```bash
GET http://localhost:8080/
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "service": "GPU Utilization Monitor",
  "version": "1.0.0",
  "config": {
    "prometheus_url": "http://prometheus:9090",
    "threshold": 20,
    "interval": 300,
    "time_window_min": 10
  }
}
```

### 2. æ‰‹åŠ¨è§¦å‘åˆ†æ

```bash
GET http://localhost:8080/analyze
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "status": "triggered",
  "message": "GPU analysis started in background"
}
```

### 3. è·å–åˆ†æç»“æœ

```bash
GET http://localhost:8080/results
```

å“åº”ç¤ºä¾‹ï¼š
```json
[
  {
    "gpu_id": "GPU-03cd1cf6-11f6-5ecc-feb0-af07406e1838",
    "pod": "comfyui-xuanwu-deploy-694d64fb5-jqs8w",
    "namespace": "comfyui",
    "hostname": "aigc-k8s-vn-master001",
    "model_name": "Tesla V100S-PCIE-32GB",
    "gpu_util": 78.5,
    "mem_copy": 45.2,
    "mem_used_mb": 18432.5,
    "power_usage": 185.3,
    "sm_clock": 1530.0,
    "score": 68.4,
    "status": "normal"
  }
]
```

### 4. è·å–ä½åˆ©ç”¨ç‡ GPU

```bash
GET http://localhost:8080/results/low
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "count": 2,
  "threshold": 20,
  "gpus": [
    {
      "gpu_id": "GPU-xxx",
      "pod": "idle-pod",
      "namespace": "test",
      "score": 5.2,
      "status": "idle"
    }
  ]
}
```

### 5. è·å–ç»Ÿè®¡ä¿¡æ¯

```bash
GET http://localhost:8080/results/stats
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "total_gpus": 24,
  "avg_score": 45.6,
  "min_score": 5.2,
  "max_score": 92.3,
  "low_utilization_count": 3,
  "idle_count": 1,
  "by_status": {
    "idle": 1,
    "low": 2,
    "normal": 15,
    "high": 6
  }
}
```

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡è¯´æ˜

ç³»ç»Ÿä» Prometheus é‡‡é›†ä»¥ä¸‹ GPU æŒ‡æ ‡ï¼š

| æŒ‡æ ‡åç§° | è¯´æ˜ | æƒé‡ |
|---------|------|------|
| `DCGM_FI_DEV_GPU_UTIL` | GPU æ ¸å¿ƒåˆ©ç”¨ç‡ (%) | 50% |
| `DCGM_FI_DEV_MEM_COPY_UTIL` | å†…å­˜å¸¦å®½ä½¿ç”¨ç‡ (%) | 20% |
| `DCGM_FI_DEV_FB_USED` | æ˜¾å­˜ä½¿ç”¨é‡ (bytes) | 20% |
| `DCGM_FI_DEV_POWER_USAGE` | åŠŸç‡ (W) | 10% |
| `DCGM_FI_DEV_SM_CLOCK` | SM æ—¶é’Ÿé¢‘ç‡ (MHz) | è¾…åŠ© |

**æŸ¥è¯¢æ¡ä»¶**ï¼š`{pod!=""}`ï¼ˆåªç›‘æ§å·²åˆ†é…ç»™ Pod çš„ GPUï¼‰

## ğŸ”Œ æ§åˆ¶æ¥å£é›†æˆ

å½“æ£€æµ‹åˆ°ä½åˆ©ç”¨ç‡ GPU æ—¶ï¼Œç³»ç»Ÿå¯ä»¥è‡ªåŠ¨è°ƒç”¨å¤–éƒ¨æ§åˆ¶æ¥å£ï¼š

### å¯ç”¨æ§åˆ¶æ¥å£

```python
class Config:
    CONTROL_API_ENABLED = True
    CONTROL_API = "http://your-control-service/api/optimize"
```

### è¯·æ±‚æ ¼å¼

```json
{
  "gpu_id": "GPU-03cd1cf6-11f6-5ecc-feb0-af07406e1838",
  "pod": "pod-name",
  "namespace": "namespace-name",
  "hostname": "node-name",
  "score": 5.2,
  "status": "idle",
  "timestamp": "2025-10-26T10:30:00"
}
```

## ğŸ³ Docker éƒ¨ç½²

### æ„å»ºé•œåƒ

```powershell
docker build -t gpu-monitor:latest .
```

### è¿è¡Œå®¹å™¨

```powershell
docker run -d \
  --name gpu-monitor \
  -p 8080:8080 \
  -e PROM_URL=http://prometheus:9090 \
  -e THRESHOLD=20 \
  -e INTERVAL=300 \
  gpu-monitor:latest
```

### Kubernetes éƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-monitor
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpu-monitor
  template:
    metadata:
      labels:
        app: gpu-monitor
    spec:
      containers:
      - name: gpu-monitor
        image: gpu-monitor:latest
        ports:
        - containerPort: 8080
        env:
        - name: PROM_URL
          value: "http://prometheus.monitoring:9090"
        - name: THRESHOLD
          value: "20"
        - name: INTERVAL
          value: "300"
```

## âš™ï¸ é…ç½®è¯´æ˜

### æ ¸å¿ƒé…ç½®é¡¹

| é…ç½®é¡¹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-------|-------|------|
| `PROM_URL` | `http://prometheus:9090` | Prometheus åœ°å€ |
| `THRESHOLD` | `20` | ä½åˆ©ç”¨ç‡é˜ˆå€¼ï¼ˆ%ï¼‰ |
| `INTERVAL` | `300` | è½®è¯¢å‘¨æœŸï¼ˆç§’ï¼‰ |
| `MAX_WORKERS` | `8` | æœ€å¤§å¹¶å‘çº¿ç¨‹æ•° |
| `TIME_WINDOW_MIN` | `10` | æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰ |
| `STEP` | `30s` | æŸ¥è¯¢æ­¥é•¿ |

### è¯„åˆ†æƒé‡é…ç½®

```python
GPU_UTIL_WEIGHT = 0.5    # GPU æ ¸å¿ƒä½¿ç”¨ç‡æƒé‡
MEM_COPY_WEIGHT = 0.2    # å†…å­˜æ‹·è´ä½¿ç”¨ç‡æƒé‡
MEM_USED_WEIGHT = 0.2    # æ˜¾å­˜ä½¿ç”¨æƒé‡
POWER_WEIGHT = 0.1       # åŠŸç‡æƒé‡
```

## ğŸ“ æ—¥å¿—è¾“å‡º

ç³»ç»Ÿä¼šè¾“å‡ºè¯¦ç»†çš„åˆ†ææ—¥å¿—ï¼š

```
2025-10-26 10:30:00 - __main__ - INFO - ============================================================
2025-10-26 10:30:00 - __main__ - INFO - GPU Utilization Analysis Results
2025-10-26 10:30:00 - __main__ - INFO - ============================================================
2025-10-26 10:30:00 - __main__ - INFO - Namespace/Pod                            GPU                                          Score     Util   Mem(MB)   Status
2025-10-26 10:30:00 - __main__ - INFO - ------------------------------------------------------------
2025-10-26 10:30:00 - __main__ - INFO - comfyui/comfyui-xuanwu-deploy-xxx        GPU-03cd1cf6-11f6-5ecc-feb0-af07406e1838    68.4%    78.5%   18432.5  normal
2025-10-26 10:30:00 - __main__ - INFO - test/idle-pod-xxx                        GPU-abc123-...                                5.2%     2.1%      128.0  idle

2025-10-26 10:30:00 - __main__ - WARNING - âš ï¸  Found 1 GPU(s) with utilization below 20%:
2025-10-26 10:30:00 - __main__ - WARNING -   - test/idle-pod-xxx (5.2%) - GPU-abc123-...
```

## ğŸ” æ•…éšœæ’æŸ¥

### 1. è¿æ¥ Prometheus å¤±è´¥

æ£€æŸ¥ Prometheus åœ°å€é…ç½®ï¼š
```python
PROM_URL = "http://prometheus:9090"
```

æµ‹è¯•è¿æ¥ï¼š
```powershell
curl http://prometheus:9090/api/v1/status/config
```

### 2. æŸ¥è¯¢ä¸åˆ° GPU æ•°æ®

æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶ï¼š
```python
QUERY_CONDITION = '{pod!=""}'
```

æ‰‹åŠ¨æµ‹è¯•æŸ¥è¯¢ï¼š
```
http://prometheus:9090/graph
DCGM_FI_DEV_GPU_UTIL{pod!=""}
```

### 3. è¯„åˆ†å¼‚å¸¸

æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦å­˜åœ¨ï¼š
- `DCGM_FI_DEV_GPU_UTIL`
- `DCGM_FI_DEV_MEM_COPY_UTIL`
- `DCGM_FI_DEV_FB_USED`
- `DCGM_FI_DEV_POWER_USAGE`

## ğŸš€ æ‰©å±•åŠŸèƒ½

### 1. æ¥å…¥ AI åˆ¤æ–­

å¯ä»¥å°†æŒ‡æ ‡æ•°æ®ä¼ é€’ç»™ AI æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ¤æ–­ï¼š

```python
def ai_judge_utilization(metrics_series):
    """
    ä½¿ç”¨ AI æ¨¡å‹åˆ¤æ–­ GPU åˆ©ç”¨ç‡
    metrics_series: æœ€è¿‘10åˆ†é’Ÿçš„æŒ‡æ ‡æ—¶åºæ•°æ®
    """
    prompt = f"""
    æ ¹æ®ä»¥ä¸‹ GPU æŒ‡æ ‡åˆ¤æ–­æ˜¯å¦åœ¨æœ‰æ•ˆä½¿ç”¨ï¼š
    - GPU åˆ©ç”¨ç‡åºåˆ—: {metrics_series['gpu_util']}
    - æ˜¾å­˜ä½¿ç”¨åºåˆ—: {metrics_series['mem_used']}
    - åŠŸç‡åºåˆ—: {metrics_series['power']}
    
    è¯·è¾“å‡º: {{"status": "active|idle", "confidence": 0-1, "reason": "..."}}
    """
    # è°ƒç”¨ LLM API
    response = call_llm(prompt)
    return response
```

### 2. Grafana å¯è§†åŒ–

å¯¼å…¥ Dashboard æ¨¡æ¿å±•ç¤ºï¼š
- GPU åˆ©ç”¨ç‡è¶‹åŠ¿
- ç§Ÿæˆ·ä½¿ç”¨æƒ…å†µ
- ä½åˆ©ç”¨ç‡å‘Šè­¦

### 3. é€šçŸ¥é›†æˆ

æ·»åŠ å‘Šè­¦é€šçŸ¥ï¼š
```python
def send_notification(low_util_gpus):
    # é£ä¹¦é€šçŸ¥
    # Slack é€šçŸ¥
    # é‚®ä»¶é€šçŸ¥
    pass
```

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜è¯·è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

