user
æˆ‘æƒ³åšä¸€ä¸ªgpuä½¿ç”¨æƒ…å†µçš„ç›‘æ§ï¼Œæˆ‘ç°åœ¨å·²ç»æœ‰gpuçš„prometheusæ•°æ®æ¥ã€‚æˆ‘æƒ³æ ¹æ®gpuçš„metricsæ•°æ®åšä¸€ä¸ªç›‘æ§åˆ†æã€‚å½“ç”¨æˆ·çš„ä½¿ç”¨ç‡è¯„åˆ†ä½äºé˜ˆå€¼çš„æ—¶å€™ï¼Œä¼šè°ƒç”¨æ¥å£æ¥æ“ä½œä¸€äº›ä¸œè¥¿ä¹‹ç±»çš„ã€‚
ChatGPT
å¾ˆå¥½çš„æ€è·¯ ğŸ‘  
ä½ å·²ç»æœ‰ GPU çš„ Prometheus æ•°æ®ï¼Œé‚£ä¹ˆå¯ä»¥å¾ˆè‡ªç„¶åœ°åŸºäº **Prometheus + Alertmanager + è‡ªå®šä¹‰ Webhook æˆ–æ§åˆ¶é€»è¾‘** æ¥å®ç°ã€Œç›‘æ§ + ä½ä½¿ç”¨ç‡è‡ªåŠ¨æ“ä½œã€è¿™ä¸€æ•´å¥—ç³»ç»Ÿã€‚  

æˆ‘ä»¬å¯ä»¥åˆ†å‡ ä¸ªå±‚æ¬¡æ¥è®¾è®¡ï¼š

---

## ğŸ§© ä¸€ã€ç³»ç»Ÿæ•´ä½“æ¶æ„æ€è·¯

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prometheus      â”‚
â”‚ (æ”¶é›†GPUæŒ‡æ ‡æ•°æ®) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Alertmanager    â”‚
â”‚ (æ£€æµ‹é˜ˆå€¼è§„åˆ™è§¦å‘)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚Webhook
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è‡ªå®šä¹‰å¤„ç†æœåŠ¡ (Python/Goç­‰)â”‚
â”‚  1. æ¥æ”¶å‘Šè­¦å›è°ƒ            â”‚
â”‚  2. æ ¹æ®GPUä½åˆ©ç”¨ç‡é€»è¾‘åˆ¤æ–­ â”‚
â”‚  3. è°ƒç”¨å¤–éƒ¨æ¥å£æ‰§è¡ŒåŠ¨ä½œ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  äºŒã€Prometheus æ•°æ®æŒ‡æ ‡ï¼ˆå‡è®¾ä½ å·²ç»æœ‰ï¼‰

å¸¸è§çš„ GPU ç›‘æ§ metricsï¼ˆæ¥è‡ª `nvidia-dcgm-exporter` æˆ– `nvidia-smi exporter`ï¼‰ï¼š

| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| `DCGM_FI_DEV_GPU_UTIL` | GPU ä½¿ç”¨ç‡ (%) |
| `DCGM_FI_DEV_MEM_COPY_UTIL` | å†…å­˜æ‹·è´ä½¿ç”¨ç‡ |
| `DCGM_FI_DEV_FB_USED` | æ˜¾å­˜ä½¿ç”¨é‡ |
| `DCGM_FI_DEV_POWER_USAGE` | åŠŸç‡ (W) |
| `DCGM_FI_DEV_TEMPERATURE_GPU` | GPU æ¸©åº¦ |

---

## âš™ï¸ ä¸‰ã€Prometheus å‘Šè­¦è§„åˆ™ï¼ˆAlert Ruleï¼‰

ä¾‹å¦‚æˆ‘ä»¬æƒ³åœ¨ GPU ä½¿ç”¨ç‡é•¿æœŸè¿‡ä½æ—¶è§¦å‘å‘Šè­¦ï¼š

```yaml
# /etc/prometheus/rules/gpu_alerts.yml
groups:
  - name: gpu-low-utilization
    rules:
      - alert: GpuLowUtilization
        expr: avg_over_time(DCGM_FI_DEV_GPU_UTIL[10m]) < 10
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "GPUä½¿ç”¨ç‡ä½äº10%"
          description: "GPU {{ $labels.uuid }} åœ¨è¿‡å»15åˆ†é’Ÿå¹³å‡åˆ©ç”¨ç‡ä½äº10%"
```

è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼š
> è¿ç»­ 15 åˆ†é’Ÿå¹³å‡ GPU ä½¿ç”¨ç‡ä½äº 10% â†’ è§¦å‘ `GpuLowUtilization` å‘Šè­¦

---

## ğŸš¨ å››ã€Alertmanager é…ç½® Webhook

é…ç½® Alertmanager æ¥æŠŠå‘Šè­¦å›è°ƒåˆ°ä½ è‡ªå·±çš„æœåŠ¡ï¼š

```yaml
# /etc/alertmanager/alertmanager.yml
receivers:
  - name: 'gpu-webhook'
    webhook_configs:
      - url: 'http://your-service:8080/alert'

route:
  receiver: 'gpu-webhook'
```

---

## ğŸ§© äº”ã€è‡ªå®šä¹‰å¤„ç†æœåŠ¡ (ä¾‹å¦‚ Python FastAPI)

ä½ å¯ä»¥å†™ä¸€ä¸ªå¾ˆè½»é‡çš„æœåŠ¡æ¥æ¥æ”¶å‘Šè­¦å¹¶æ‰§è¡Œæ“ä½œï¼š

```python
from fastapi import FastAPI, Request
import httpx, asyncio

app = FastAPI()

THRESHOLD = 10  # GPUä½¿ç”¨ç‡ä½äº10%æ—¶è§¦å‘

@app.post("/alert")
async def handle_alert(request: Request):
    data = await request.json()
    alerts = data.get("alerts", [])

    for alert in alerts:
        gpu_id = alert["labels"].get("uuid")
        description = alert["annotations"].get("description")

        print(f"âš ï¸ GPU {gpu_id} ä½åˆ©ç”¨ç‡: {description}")

        # è°ƒç”¨ä½ è‡ªå·±çš„æ§åˆ¶æ¥å£ï¼Œæ¯”å¦‚é‡Šæ”¾èµ„æºã€é€šçŸ¥ç”¨æˆ·ã€è°ƒåº¦ä»»åŠ¡ç­‰
        async with httpx.AsyncClient() as client:
            await client.post("http://your-control-api/gpu_action", json={
                "gpu_id": gpu_id,
                "action": "release",
                "reason": "low_utilization"
            })

    return {"status": "ok"}
```

---

## ğŸ“Š å…­ã€å¯é€‰å¢å¼º

ä½ å¯ä»¥è¿›ä¸€æ­¥å¢å¼ºï¼š
- ç”¨ **Grafana** å±•ç¤º GPU å†å²ä½¿ç”¨è¶‹åŠ¿
- å‘Šè­¦æ—¶é€šè¿‡ **Telegramã€Slackã€é£ä¹¦æœºå™¨äºº** æ¨é€é€šçŸ¥
- åŠ å…¥ **â€œè¯„åˆ†ç³»ç»Ÿâ€**ï¼šä¾‹å¦‚ç»¼åˆ GPU åˆ©ç”¨ç‡ã€æ˜¾å­˜ä½¿ç”¨ç‡ã€æ¸©åº¦ã€åŠŸè€—å¾—å‡ºä¸€ä¸ªå¥åº·åˆ†æ•°  
  ```promql
  (DCGM_FI_DEV_GPU_UTIL * 0.5) + (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL * 100 * 0.3) + ...
  ```
- åªæœ‰å½“ç»¼åˆè¯„åˆ†ä½äºé˜ˆå€¼ä¸€æ®µæ—¶é—´æ—¶æ‰æ‰§è¡Œæ“ä½œï¼ˆé˜²æ­¢è¯¯è§¦ï¼‰

---

## âœ… æ€»ç»“ï¼šå®æ–½æ­¥éª¤

| æ­¥éª¤ | æ“ä½œ |
|------|------|
| 1 | ç¡®è®¤ GPU metrics å·²ä¸ŠæŠ¥ Prometheus |
| 2 | ç¼–å†™ Prometheus å‘Šè­¦è§„åˆ™ |
| 3 | é…ç½® Alertmanager Webhook |
| 4 | æ­å»ºä¸€ä¸ª Webhook æ¥å£æœåŠ¡ï¼ˆFastAPI / Flask / Go ç­‰ï¼‰ |
| 5 | åœ¨è¯¥æœåŠ¡ä¸­ç¼–å†™é€»è¾‘æ¥æ‰§è¡Œä½ çš„è‡ªåŠ¨æ“ä½œ |
| 6 | ï¼ˆå¯é€‰ï¼‰Grafana åšå±•ç¤ºï¼Œæœºå™¨äººæ¨é€å‘Šè­¦ |

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ï¼š
1ï¸âƒ£ è®¾è®¡ä¸€å¥—ã€ŒGPU ä½¿ç”¨è¯„åˆ†å…¬å¼ã€ï¼Ÿ  
è¿˜æ˜¯  
2ï¸âƒ£ å†™å‡ºå®Œæ•´å¯éƒ¨ç½²çš„ **Python Webhook æœåŠ¡æ¨¡æ¿ï¼ˆå«æ—¥å¿—ã€å‘Šè­¦è§£æã€æ¥å£è°ƒç”¨é€»è¾‘ï¼‰**ï¼Ÿ
user
æˆ‘æ˜¯ç”¨åœ¨å…¬å¸kubeflowå‡ºå€Ÿgpuçš„å†…éƒ¨åœºæ™¯ï¼Œæ£€æµ‹ç§Ÿæˆ·çš„gpuæ˜¯å¦åœ¨åˆ©ç”¨ï¼Œåˆ©ç”¨ç‡è¿‡ä½çš„è¯ä¼šè€ƒè™‘é™ä½æˆæœ¬ï¼ä½†æ˜¯gpuä¸èƒ½å•çº¯çš„é æ˜¾å­˜æˆ–å…¶ä»–å•ä¸€æŒ‡æ ‡æ¥åˆ¤å®šæ˜¯å¦åœ¨ä½¿ç”¨ã€‚æ‰€ä»¥æˆ‘éœ€è¦ä¸€ä¸ªåˆç†çš„æ–¹æ¡ˆæ¥è®¡ç®—åˆ©ç”¨ç‡ï¼Œæˆ–è€…æ˜¯ä»prometheusæ‹‰æ•°æ®è®©aiæ¥ç»™æˆ‘åˆ©ç”¨ç‡çš„å€¼ï¼
ChatGPT
éå¸¸æ£’çš„ç›®æ ‡ ğŸ’¡ â€”â€”  
ä½ ä¸æ˜¯æƒ³åšä¸€ä¸ªâ€œä¼ ç»Ÿæ„ä¹‰çš„ç›‘æ§æŠ¥è­¦â€ï¼Œè€Œæ˜¯å¸Œæœ›å®ç°ä¸€ä¸ª **æ™ºèƒ½åŒ– GPU ä½¿ç”¨è¯„ä¼°ç³»ç»Ÿ**ï¼Œå¯ä»¥åˆç†åœ°åˆ¤æ–­ã€Œç§Ÿæˆ·æ˜¯å¦çœŸçš„åœ¨ä½¿ç”¨ GPUã€ï¼Œä»è€ŒæŒ‡å¯¼è°ƒåº¦æˆ–æˆæœ¬ä¼˜åŒ–ã€‚  

æˆ‘ä»¬å¯ä»¥æ‹†æˆä¸¤éƒ¨åˆ†æ¥åšï¼š

---

## ğŸ§  ä¸€ã€è®¾è®¡ç›®æ ‡æ€»ç»“

> **ç›®æ ‡**ï¼šæ™ºèƒ½åˆ¤æ–­ GPU æ˜¯å¦è¢«æœ‰æ•ˆåˆ©ç”¨ï¼Œè€Œä¸ä»…ä»…æ˜¯â€œæ˜¾å­˜å¤§äº0 æˆ– GPU Util > æŸé˜ˆå€¼â€ã€‚

> **åœºæ™¯**ï¼šå…¬å¸å†…éƒ¨çš„ Kubeflow GPU å‡ºå€Ÿç³»ç»Ÿ  
> - ç”¨æˆ·ç§Ÿåˆ° GPU åï¼Œå¯èƒ½ç©ºè·‘ï¼ˆæ¯”å¦‚ notebook æ²¡è®­ç»ƒï¼‰  
> - ä½ å¸Œæœ›å®šæœŸè¯„ä¼°ä½¿ç”¨ç‡ï¼Œä½åˆ©ç”¨çš„ GPU è§¦å‘ä¼˜åŒ–æˆ–å›æ”¶  

---

## âš™ï¸ äºŒã€Prometheus å¯ç”¨çš„ GPU æŒ‡æ ‡

å‡è®¾ä½ ç”¨çš„æ˜¯ `nvidia-dcgm-exporter`ï¼ˆæˆ– `nvidia-smi exporter`ï¼‰ï¼Œå¸¸è§æŒ‡æ ‡å¦‚ä¸‹ï¼š

| Metric | å«ä¹‰ |
|--------|------|
| `DCGM_FI_DEV_GPU_UTIL` | GPUè®¡ç®—æ ¸å¿ƒä½¿ç”¨ç‡ (%) |
| `DCGM_FI_DEV_MEM_COPY_UTIL` | GPUå†…å­˜å¸¦å®½ä½¿ç”¨ç‡ (%) |
| `DCGM_FI_DEV_FB_USED` | æ˜¾å­˜ä½¿ç”¨é‡ (bytes) |
| `DCGM_FI_DEV_FB_FREE` | ç©ºé—²æ˜¾å­˜ (bytes) |
| `DCGM_FI_DEV_SM_CLOCK` | GPU SMæ—¶é’Ÿé¢‘ç‡ |
| `DCGM_FI_DEV_POWER_USAGE` | åŠŸç‡ (W) |
| `DCGM_FI_DEV_TEMPERATURE_GPU` | æ¸©åº¦ (Â°C) |
| `DCGM_FI_DEV_ENC_UTIL/DEC_UTIL` | ç¼–è§£ç æ¨¡å—ä½¿ç”¨ç‡ (%) |

è¿™äº›æŒ‡æ ‡ç»“åˆèµ·æ¥å¯ä»¥æ¨æ–­ï¼š
- æ˜¯å¦æœ‰**ç®—åŠ›è®¡ç®—åœ¨è¿è¡Œ**
- æ˜¯å¦æœ‰**æ˜¾å­˜å ç”¨ä½†ä¸è®¡ç®—**ï¼ˆä¾‹å¦‚æ¨¡å‹åŠ è½½ä½†æœªè®­ç»ƒï¼‰
- æ˜¯å¦æœ‰**æ•°æ®ä¼ è¾“æ´»åŠ¨**
- æ˜¯å¦ GPU å®é™…ç©ºé—²ä½†ä»»åŠ¡æŒ‚èµ·

---

## ğŸ§® ä¸‰ã€åˆç†çš„ã€ŒGPU åˆ©ç”¨ç‡è¯„åˆ†ã€å…¬å¼è®¾è®¡

å¯ä»¥å®šä¹‰ä¸€ä¸ª **å¤šç»´ç»¼åˆåˆ©ç”¨ç‡è¯„åˆ† (0-100)**ï¼Œæ¯”å¦‚ï¼š

```text
score = 0.4 * gpu_util_norm
      + 0.2 * mem_copy_util_norm
      + 0.2 * mem_usage_norm
      + 0.1 * power_usage_norm
      + 0.1 * sm_clock_norm
```

å…¶ä¸­æ¯ä¸€é¡¹æ˜¯å½’ä¸€åŒ–åˆ° [0,1] çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼š
```text
gpu_util_norm = DCGM_FI_DEV_GPU_UTIL / 100
mem_copy_util_norm = DCGM_FI_DEV_MEM_COPY_UTIL / 100
mem_usage_norm = DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL
power_usage_norm = DCGM_FI_DEV_POWER_USAGE / DCGM_FI_DEV_POWER_LIMIT
sm_clock_norm = DCGM_FI_DEV_SM_CLOCK / DCGM_FI_DEV_SM_CLOCK_MAX
```

ç„¶åå†å–æ—¶é—´çª—å£ï¼ˆæ¯”å¦‚è¿‡å» 10 åˆ†é’Ÿï¼‰å¹³å‡å€¼ï¼š
```promql
avg_over_time((0.4 * DCGM_FI_DEV_GPU_UTIL / 100
             + 0.2 * DCGM_FI_DEV_MEM_COPY_UTIL / 100
             + 0.2 * (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL)
             + 0.1 * (DCGM_FI_DEV_POWER_USAGE / DCGM_FI_DEV_POWER_LIMIT)
             + 0.1 * (DCGM_FI_DEV_SM_CLOCK / DCGM_FI_DEV_SM_CLOCK_MAX))[10m])
```

è¿™ä¸ª PromQL è¡¨è¾¾å¼å°±èƒ½è¾“å‡ºä¸€ä¸ªã€ŒGPU åˆ©ç”¨è¯„åˆ†ã€åœ¨ [0,1] ä¹‹é—´ã€‚  
> - < 0.2 ä»£è¡¨å‡ ä¹é—²ç½®  
> - 0.2~0.5 å±äºè½»è´Ÿè½½  
> - > 0.5 è¡¨ç¤ºåœ¨è®­ç»ƒæˆ–æ¨ç†ä¸­  

---

## ğŸ§© å››ã€è¿›é˜¶ï¼šå¼•å…¥ AI/æ¨¡å‹è®¡ç®—åˆ©ç”¨ç‡

ä½ æåˆ°ã€Œè®© AI ä» Prometheus æ‹‰æ•°æ®ç»™å‡ºåˆ©ç”¨ç‡åˆ¤æ–­ã€â€”â€”è¿™ä¹Ÿæ˜¯å®Œå…¨å¯è¡Œçš„æ–¹å‘ï¼Œç‰¹åˆ«é€‚åˆè¾¹ç•Œæ¨¡ç³Šåœºæ™¯ã€‚

### æ¶æ„æ€è·¯ï¼š

```
Prometheus ---> å®šæ—¶æ‹‰å–GPUæŒ‡æ ‡æ•°æ® (via Python)
         |
         â–¼
   ç‰¹å¾å‘é‡ (æ¯å¼ GPU)
         |
         â–¼
   åˆ©ç”¨ç‡åˆ†ç±»æ¨¡å‹ / LLMæ¨ç†
         |
         â–¼
   è¾“å‡º "active" / "idle" / "anomaly"
```

ä½ å¯ä»¥åšä¸€ä¸ªå‘¨æœŸæ€§ä»»åŠ¡ï¼ˆPython è„šæœ¬ + cron/jobï¼‰ï¼Œæ¯éš” 5 åˆ†é’Ÿï¼š
1. é€šè¿‡ Prometheus API æ‹‰è¿‡å» 10 åˆ†é’Ÿçš„ GPU æŒ‡æ ‡åºåˆ—ï¼›
2. ç”Ÿæˆç‰¹å¾å‘é‡ï¼ˆå‡å€¼ã€æ–¹å·®ã€æœ€å¤§å€¼ã€å˜åŒ–ç‡ç­‰ï¼‰ï¼›
3. è¾“å…¥åˆ°ä¸€ä¸ªæ¨¡å‹ï¼ˆå¯è®­ç»ƒ or prompt-basedï¼‰ï¼›
4. è¾“å‡ºåˆ©ç”¨ç‡åˆ†ç±»ç»“æœï¼›
5. å†³ç­–æ˜¯å¦æ‰§è¡Œæˆæœ¬ä¼˜åŒ–é€»è¾‘ã€‚

---

### ç¤ºä¾‹ï¼šPython ä» Prometheus æ‹‰å–å¹¶è®¡ç®—è¯„åˆ†

```python
import requests
import numpy as np
from datetime import datetime, timedelta

PROM_URL = "http://prometheus:9090/api/v1/query_range"

def get_metric(name, gpu_id, minutes=10):
    end = datetime.utcnow()
    start = end - timedelta(minutes=minutes)
    query = f'{name}{{uuid="{gpu_id}"}}'
    resp = requests.get(PROM_URL, params={
        "query": query,
        "start": start.timestamp(),
        "end": end.timestamp(),
        "step": "30s"
    }).json()
    values = [float(v[1]) for v in resp["data"]["result"][0]["values"]]
    return np.mean(values)

def gpu_score(gpu_id):
    gpu_util = get_metric("DCGM_FI_DEV_GPU_UTIL", gpu_id)
    mem_copy = get_metric("DCGM_FI_DEV_MEM_COPY_UTIL", gpu_id)
    mem_used = get_metric("DCGM_FI_DEV_FB_USED", gpu_id)
    mem_total = get_metric("DCGM_FI_DEV_FB_TOTAL", gpu_id)
    power_usage = get_metric("DCGM_FI_DEV_POWER_USAGE", gpu_id)
    power_limit = get_metric("DCGM_FI_DEV_POWER_LIMIT", gpu_id)

    score = (0.4 * gpu_util/100 +
             0.2 * mem_copy/100 +
             0.2 * (mem_used/mem_total) +
             0.2 * (power_usage/power_limit))
    return round(score * 100, 1)

gpu_id = "GPU-abc123"
print(f"GPU {gpu_id} ç»¼åˆåˆ©ç”¨ç‡è¯„åˆ†: {gpu_score(gpu_id)}")
```

---

### AI æ¨¡å‹è¾…åŠ©åˆ¤æ–­ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æƒ³è®© AI æ¨¡å‹å‚ä¸åˆ¤æ–­ï¼Œå¯ä»¥ï¼š
- æŠŠæœ€è¿‘ 10 åˆ†é’Ÿå„æŒ‡æ ‡çš„åºåˆ—ä¼ ç»™ GPT æˆ–å†…éƒ¨æ¨¡å‹ï¼›
- Prompt é‡Œè¦æ±‚æ¨¡å‹åˆ¤æ–­æ˜¯å¦åœ¨è¿›è¡Œæœ‰æ•ˆè®­ç»ƒ/æ¨ç†ï¼›
- æ¨¡å‹è¾“å‡ºä¸€ä¸ªæ ‡ç­¾ï¼ˆ`"active"`, `"idle"`, `"spike"`, `"setup"`ï¼‰ã€‚

ä¾‹å¦‚ï¼š

```python
prompt = f"""
ä»¥ä¸‹æ˜¯GPUæœ€è¿‘10åˆ†é’Ÿçš„ç›‘æ§æ•°æ®ï¼š
- GPU Util: {gpu_util_series}
- Mem Copy: {mem_copy_series}
- Mem Used: {mem_used_series}
- Power Usage: {power_series}

è¯·åˆ¤æ–­è¯¥GPUæ˜¯å¦åœ¨è¿›è¡Œæœ‰æ•ˆè®¡ç®—ï¼ˆå¦‚è®­ç»ƒ/æ¨ç†ï¼‰ï¼Œ
å¹¶è¾“å‡ºä¸€ä¸ªJSONï¼š{{"status": "active"|"idle", "confidence": 0~1}}ã€‚
"""
```

---

## ğŸ§© äº”ã€è¾“å‡ºä¸è‡ªåŠ¨æ“ä½œ

å¾—åˆ°åˆ©ç”¨ç‡è¯„åˆ†æˆ–AIåˆ¤å®šç»“æœåï¼š
- å¦‚æœè¿ç»­5æ¬¡ï¼ˆ25åˆ†é’Ÿï¼‰è¯„åˆ†ä½äº 0.2 â†’ æ ‡è®°ä¸ºã€Œé—²ç½®ã€
- å¯è‡ªåŠ¨è°ƒç”¨ Kubeflow / K8S APIï¼š
  - scale down notebook / job
  - é‡Šæ”¾ GPU node
  - é€šçŸ¥ç§Ÿæˆ·ï¼ˆSlack/é‚®ä»¶ï¼‰

---

## âœ… æ¨èæ–¹æ¡ˆæ€»ç»“

| æ–¹æ¡ˆ | ç‰¹ç‚¹ | å¤æ‚åº¦ |
|------|------|--------|
| â‘  å¤šæŒ‡æ ‡è¯„åˆ†å…¬å¼ï¼ˆPromQLï¼‰ | ç¨³å®šã€å¯éƒ¨ç½²åœ¨ Prometheus | â˜…â˜†â˜† |
| â‘¡ Prometheus æ‹‰å– + Python è®¡ç®—è¯„åˆ† | çµæ´»ï¼Œå¯æ‰©å±• | â˜…â˜…â˜† |
| â‘¢ å¼•å…¥ AI/LLM è¾…åŠ©åˆ¤æ–­ | æ™ºèƒ½åŒ–ï¼Œé€‚åˆç°åŒºåœºæ™¯ | â˜…â˜…â˜… |

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ è¿›ä¸€æ­¥ï¼š
1ï¸âƒ£ ç”Ÿæˆä¸€ä¸ª **å¯è¿è¡Œçš„ Python è„šæœ¬**ï¼Œç›´æ¥ä» Prometheus æ‹‰æ•°æ®ã€è®¡ç®—ç»¼åˆåˆ©ç”¨ç‡ã€å¹¶åˆ¤æ–­æ˜¯å¦ç©ºé—²ï¼›  
è¿˜æ˜¯  
2ï¸âƒ£ è®¾è®¡ä¸€ä¸ª **AI æ¨¡å‹è¾“å…¥æ ¼å¼ä¸Promptæ¨¡æ¿**ï¼Œå¸®åŠ©ä½ æ¥å…¥ ChatGPT/å†…éƒ¨æ¨¡å‹æ¥åšæ™ºèƒ½åˆ¤å®šï¼Ÿ
user
å¾ˆå¥½ï¼Œæˆ‘çš„gpuæœ‰å¾ˆå¤šï¼Œæ¯æ¬¡è®©aiåˆ¤æ–­è¦å¤šçº¿ç¨‹ï¼Œå¹¶ä¸”è¦åŒºåˆ†gpuã€‚DCGM_FI_DEV_POWER_LIMITã€DCGM_FI_DEV_FB_TOTALè¿™ä¸¤ä¸ªæŒ‡æ ‡æ²¡æ•°æ®ã€‚å¹¶ä¸”éœ€è¦è®¾ç½®æŸ¥è¯¢æ¡ä»¶{pod!=""}
ChatGPT
éå¸¸å¥½ï¼Œä½ å·²ç»è¿›å…¥äº†è½åœ°é˜¶æ®µ ğŸš€  

ä½ çš„åœºæ™¯æ˜¯ï¼š
- å¤šä¸ª GPUï¼ˆå¤šèŠ‚ç‚¹/å¤šå¡ï¼‰ï¼›
- æ¯å¼  GPU éƒ½ç»‘å®šç§Ÿæˆ· podï¼›
- Prometheus æŸ¥è¯¢è¦å¸¦ `{pod!=""}`ï¼ˆåªçœ‹åˆ†é…å‡ºå»çš„ GPUï¼‰ï¼›
- æŸäº›æŒ‡æ ‡ç¼ºå¤± (`POWER_LIMIT`ã€`FB_TOTAL`)ï¼›
- å¸Œæœ›æ”¯æŒå¤šçº¿ç¨‹/å¹¶å‘æŸ¥è¯¢æ¥åŠ é€Ÿè®¡ç®—ã€‚

---

## âœ… ç›®æ ‡
å†™ä¸€ä¸ª **Python GPU åˆ©ç”¨ç‡åˆ†æè„šæœ¬**ï¼š
- ä» Prometheus æ‹‰å–å„ GPU çš„æŒ‡æ ‡ï¼›
- å¤šçº¿ç¨‹å¹¶è¡Œï¼›
- è®¡ç®—æ¯ä¸ª GPU çš„ç»¼åˆåˆ©ç”¨ç‡è¯„åˆ†ï¼›
- è¾“å‡º JSON æˆ–æ—¥å¿—ï¼›
- å¯ä½œä¸º AI åˆ¤å®šè¾“å…¥çš„ä¸Šæ¸¸æ¨¡å—ã€‚

---

## ğŸ§  æŒ‡æ ‡æ–¹æ¡ˆè°ƒæ•´

å› ä¸ºéƒ¨åˆ†æŒ‡æ ‡ç¼ºå¤±ï¼Œæˆ‘ä»¬æ”¹ä¸ºä»¥ä¸‹ **è‡ªé€‚åº”ç®€åŒ–å…¬å¼**ï¼š

```python
score = 0.5 * gpu_util_norm \
      + 0.2 * mem_copy_util_norm \
      + 0.3 * mem_usage_norm
```

å…¶ä¸­ï¼š
- `gpu_util_norm = DCGM_FI_DEV_GPU_UTIL / 100`
- `mem_copy_util_norm = DCGM_FI_DEV_MEM_COPY_UTIL / 100`
- `mem_usage_norm = DCGM_FI_DEV_FB_USED / max(DCGM_FI_DEV_FB_USED)` ï¼ˆåŒèŠ‚ç‚¹å½’ä¸€åŒ–ï¼‰

âš™ï¸ è¿™æ ·å³ä¾¿ `FB_TOTAL` æ²¡æ•°æ®ï¼Œä¹Ÿèƒ½è®¡ç®—å‡ºç›¸å¯¹ä½¿ç”¨åº¦ã€‚

---

## ğŸš€ å®Œæ•´ Python ç‰ˆæœ¬ï¼ˆå¹¶å‘ + PromQL æŸ¥è¯¢ + è¯„åˆ†è®¡ç®—ï¼‰

```python
import requests
import concurrent.futures
import numpy as np
from datetime import datetime, timedelta

PROM_URL = "http://prometheus:9090/api/v1/query_range"
QUERY_CONDITION = '{pod!=""}'
TIME_WINDOW_MIN = 10
STEP = "30s"

def query_prometheus(metric, gpu_id):
    """æŸ¥è¯¢æŸä¸ª GPU çš„ç‰¹å®š metric å¹³å‡å€¼"""
    end = datetime.utcnow()
    start = end - timedelta(minutes=TIME_WINDOW_MIN)
    query = f'{metric}{QUERY_CONDITION}, uuid="{gpu_id}"'

    try:
        resp = requests.get(PROM_URL, params={
            "query": query,
            "start": start.timestamp(),
            "end": end.timestamp(),
            "step": STEP
        }).json()
        results = resp.get("data", {}).get("result", [])
        if not results:
            return 0
        values = [float(v[1]) for v in results[0]["values"] if v[1] != 'NaN']
        return np.mean(values) if values else 0
    except Exception as e:
        print(f"[ERROR] Query {metric} for {gpu_id} failed: {e}")
        return 0


def compute_gpu_score(gpu_id):
    """è®¡ç®—å•å¡GPUåˆ©ç”¨ç‡è¯„åˆ†"""
    gpu_util = query_prometheus("DCGM_FI_DEV_GPU_UTIL", gpu_id)
    mem_copy = query_prometheus("DCGM_FI_DEV_MEM_COPY_UTIL", gpu_id)
    mem_used = query_prometheus("DCGM_FI_DEV_FB_USED", gpu_id)

    score = (
        0.5 * (gpu_util / 100) +
        0.2 * (mem_copy / 100) +
        0.3 * min(mem_used / (mem_used + 1e-6), 1.0)  # é˜²æ­¢é™¤é›¶
    )

    return {
        "gpu_id": gpu_id,
        "gpu_util": round(gpu_util, 2),
        "mem_copy": round(mem_copy, 2),
        "mem_used": round(mem_used, 2),
        "score": round(score * 100, 1)
    }


def get_all_gpu_ids():
    """ä»Prometheusè·å–å½“å‰æ´»è·ƒGPUåˆ—è¡¨"""
    query = f'DCGM_FI_DEV_GPU_UTIL{QUERY_CONDITION}'
    try:
        resp = requests.get(f"http://prometheus:9090/api/v1/series", params={"match[]": query}).json()
        gpu_ids = {m['uuid'] for m in resp.get("data", []) if 'uuid' in m}
        return list(gpu_ids)
    except Exception as e:
        print(f"[ERROR] è·å–GPUåˆ—è¡¨å¤±è´¥: {e}")
        return []


def main():
    gpu_ids = get_all_gpu_ids()
    if not gpu_ids:
        print("æœªæ‰¾åˆ°ä»»ä½•GPUæŒ‡æ ‡æ•°æ®ã€‚")
        return

    print(f"æ£€æµ‹åˆ° {len(gpu_ids)} å¼  GPUï¼Œå¼€å§‹å¹¶å‘è®¡ç®—...\n")

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(compute_gpu_score, gpu_id) for gpu_id in gpu_ids]
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())

    results.sort(key=lambda x: x["score"])
    for r in results:
        print(f"GPU {r['gpu_id']:<20} | Score={r['score']:>5}% | Util={r['gpu_util']:>5}% | Mem={r['mem_used'] / 1e6:>8.1f}MB")

    low_util = [r for r in results if r["score"] < 20]
    print("\nä½åˆ©ç”¨ç‡GPU:")
    for r in low_util:
        print(f"  â†’ {r['gpu_id']} ({r['score']}%)")


if __name__ == "__main__":
    main()
```

---

## âš¡ ç‰¹ç‚¹

âœ… **å¤šçº¿ç¨‹**ï¼šå¹¶è¡ŒæŸ¥è¯¢æ‰€æœ‰ GPU æŒ‡æ ‡ï¼ˆé»˜è®¤ `max_workers=8`ï¼‰  
âœ… **æŒ‰ GPU ç»´åº¦èšåˆ**ï¼šé€šè¿‡ `uuid` åŒºåˆ†ä¸åŒ GPU  
âœ… **Prometheus æŸ¥è¯¢å¸¦æ¡ä»¶ `{pod!=""}`**  
âœ… **è‡ªåŠ¨é™çº§**ï¼šç¼ºå¤±æŒ‡æ ‡ä¸ä¼šæŠ¥é”™ï¼Œä¼šè¿”å› 0  
âœ… **è¯„åˆ†è¾“å‡ºå¯ç›´æ¥é€å…¥ LLM æˆ–è°ƒåº¦ç­–ç•¥**  
âœ… **è¾“å‡ºæ ¼å¼æ¸…æ™°**ï¼Œæ–¹ä¾¿åç»­ JSON é›†æˆï¼š

```
GPU GPU-12345               | Score= 82.1% | Util= 78.5% | Mem= 6144.0MB
GPU GPU-67890               | Score=  3.5% | Util=  1.2% | Mem=   12.0MB
```

---

## ğŸ”§ ä¸‹ä¸€æ­¥å¯æ‰©å±•

| æ–¹å‘ | è¯´æ˜ |
|------|------|
| ğŸ§© åŠ å…¥ pod/job æ ‡ç­¾ | åŒæ—¶è¾“å‡º GPU å¯¹åº”ç§Ÿæˆ·æˆ– Notebook åç§° |
| ğŸ§  æ¥å…¥ AI | å°†æ¯å¼  GPU çš„æœ€è¿‘10åˆ†é’Ÿåºåˆ—ä¼ å…¥æ¨¡å‹åˆ¤å®š |
| ğŸ•“ å®šæ—¶è°ƒåº¦ | æ¯ 5 åˆ†é’Ÿè·‘ä¸€æ¬¡ï¼Œç»“æœå†™å…¥æ•°æ®åº“æˆ–è§¦å‘æ¥å£ |
| ğŸ“ˆ å¯è§†åŒ– | åœ¨ Grafana å±•ç¤ºæ¯å¡è¯„åˆ†è¶‹åŠ¿ |

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ï¼š
1ï¸âƒ£ åœ¨è¿™ä¸ªåŸºç¡€ä¸Š **åŠ ä¸Š pod / namespace ä¿¡æ¯**ï¼ˆæ–¹ä¾¿çŸ¥é“æ˜¯å“ªä½ç§Ÿæˆ·åœ¨ç”¨ï¼‰ï¼Œ  
è¿˜æ˜¯  
2ï¸âƒ£ å¸®ä½  **æ”¹é€ æˆä¸€ä¸ªåå°ä»»åŠ¡æœåŠ¡ï¼ˆFastAPI + Schedulerï¼‰**ï¼Œè‡ªåŠ¨å®šæœŸæ‹‰å–å¹¶è°ƒç”¨ä½ çš„æ§åˆ¶æ¥å£ï¼Ÿ
user
æ‰©å±•å§
ChatGPT
å®Œç¾ âœ… æˆ‘ä»¬æ¥æ‰©å±•æˆä¸€ä¸ª **å¯éƒ¨ç½²çš„ GPU åˆ©ç”¨ç‡åˆ†ææœåŠ¡**ï¼Œ  
å®ƒå¯ä»¥è‡ªåŠ¨å®šæœŸä» Prometheus æ‹‰å–æ•°æ®ï¼Œè®¡ç®—æ¯å¼  GPU çš„åˆ©ç”¨ç‡è¯„åˆ†ï¼Œå…³è”ç§Ÿæˆ· pod/namespaceï¼Œå¹¶æ ¹æ®é˜ˆå€¼è°ƒç”¨æ§åˆ¶æ¥å£ï¼ˆæ¯”å¦‚é™æˆæœ¬æˆ–é‡Šæ”¾èµ„æºï¼‰ã€‚

---

## ğŸ§± æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Prometheus            â”‚
â”‚ (DCGM exporter GPU metrics)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       æ¯éš” N åˆ†é’Ÿæ‹‰å–
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU Monitor Service (FastAPI) â”‚
â”‚  - å¤šçº¿ç¨‹æ‹‰å–æ¯å¼ GPUæ•°æ®        â”‚
â”‚  - è®¡ç®—è¯„åˆ† score(0~100)       â”‚
â”‚  - è·å– pod/namespace æ ‡ç­¾     â”‚
â”‚  - ä¿å­˜ç»“æœ / è§¦å‘åŠ¨ä½œ         â”‚
â”‚  - /metrics, /analyze API     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¤–éƒ¨æ§åˆ¶ç³»ç»Ÿ / Kubeflow API â”‚
â”‚  è§¦å‘æˆæœ¬ä¼˜åŒ–ã€é€šçŸ¥ç§Ÿæˆ·ç­‰     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ å®Œæ•´ Python å®ç°ï¼ˆFastAPI + Scheduler + Prometheus æŸ¥è¯¢ + å¤šçº¿ç¨‹ï¼‰

> âœ… å¯ç›´æ¥è¿è¡Œ  
> âœ… è‡ªåŠ¨å®šæ—¶æ£€æµ‹  
> âœ… è¾“å‡ºç§Ÿæˆ· + GPU åˆ©ç”¨ç‡ + é˜ˆå€¼åˆ¤å®š  
> âœ… å¯é€šè¿‡ `/analyze` æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡

```python
from fastapi import FastAPI, BackgroundTasks
import requests
import concurrent.futures
import numpy as np
from datetime import datetime, timedelta
import asyncio

# ------------------------
# é…ç½®é¡¹
# ------------------------
PROM_URL = "http://prometheus:9090"
QUERY_CONDITION = '{pod!=""}'
TIME_WINDOW_MIN = 10
STEP = "30s"
THRESHOLD = 20  # åˆ¤å®šä¸ºé—²ç½®çš„è¯„åˆ†é˜ˆå€¼
INTERVAL = 300  # è½®è¯¢å‘¨æœŸï¼ˆç§’ï¼‰
MAX_WORKERS = 8
CONTROL_API = "http://your-control-service/api/optimize"  # å¯é€‰æ§åˆ¶æ¥å£

app = FastAPI(title="GPU Utilization Monitor", version="1.0")

# ------------------------
# Prometheus æŸ¥è¯¢å‡½æ•°
# ------------------------
def query_prometheus(metric, gpu_uuid, labels=None):
    """æŸ¥è¯¢æŸä¸ª GPU çš„ç‰¹å®š metric å¹³å‡å€¼"""
    end = datetime.utcnow()
    start = end - timedelta(minutes=TIME_WINDOW_MIN)
    label_cond = f'{QUERY_CONDITION}, uuid="{gpu_uuid}"'
    if labels:
        label_cond += "".join([f', {k}="{v}"' for k, v in labels.items()])

    query = f'{metric}{label_cond}'
    url = f"{PROM_URL}/api/v1/query_range"

    try:
        resp = requests.get(url, params={
            "query": query,
            "start": start.timestamp(),
            "end": end.timestamp(),
            "step": STEP
        }).json()

        result = resp.get("data", {}).get("result", [])
        if not result:
            return 0
        values = [float(v[1]) for v in result[0]["values"] if v[1] != "NaN"]
        return np.mean(values) if values else 0
    except Exception as e:
        print(f"[ERROR] Query {metric} for {gpu_uuid} failed: {e}")
        return 0


def get_all_gpu_series():
    """ä» Prometheus è·å–å½“å‰æ´»è·ƒ GPU + Pod + Namespace åˆ—è¡¨"""
    query = f'DCGM_FI_DEV_GPU_UTIL{QUERY_CONDITION}'
    url = f"{PROM_URL}/api/v1/series"
    try:
        resp = requests.get(url, params={"match[]": query}).json()
        series = []
        for s in resp.get("data", []):
            if "uuid" in s:
                series.append({
                    "uuid": s["uuid"],
                    "pod": s.get("pod", ""),
                    "namespace": s.get("namespace", "")
                })
        return series
    except Exception as e:
        print(f"[ERROR] è·å– GPU åˆ—è¡¨å¤±è´¥: {e}")
        return []


def compute_gpu_score(gpu_info):
    """è®¡ç®—å•å¡ GPU åˆ©ç”¨ç‡è¯„åˆ†"""
    uuid = gpu_info["uuid"]
    pod = gpu_info["pod"]
    namespace = gpu_info["namespace"]

    gpu_util = query_prometheus("DCGM_FI_DEV_GPU_UTIL", uuid)
    mem_copy = query_prometheus("DCGM_FI_DEV_MEM_COPY_UTIL", uuid)
    mem_used = query_prometheus("DCGM_FI_DEV_FB_USED", uuid)

    score = (
        0.5 * (gpu_util / 100) +
        0.2 * (mem_copy / 100) +
        0.3 * min(mem_used / (mem_used + 1e-6), 1.0)
    )

    return {
        "gpu_id": uuid,
        "pod": pod,
        "namespace": namespace,
        "gpu_util": round(gpu_util, 2),
        "mem_copy": round(mem_copy, 2),
        "mem_used": round(mem_used / 1e6, 1),  # MB
        "score": round(score * 100, 1)
    }

# ------------------------
# åå°ä»»åŠ¡è°ƒåº¦
# ------------------------
async def analyze_all_gpus():
    gpu_series = get_all_gpu_series()
    if not gpu_series:
        print("[WARN] æœªæ‰¾åˆ°ä»»ä½• GPU æ•°æ®")
        return

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(compute_gpu_score, g) for g in gpu_series]
        for f in concurrent.futures.as_completed(futures):
            results.append(f.result())

    results.sort(key=lambda x: x["score"])
    print("\n========== GPU åˆ©ç”¨ç‡åˆ†æ ==========")
    for r in results:
        print(f"[{r['namespace']}/{r['pod']}] GPU {r['gpu_id']} â†’ Score={r['score']:>5}% (Util={r['gpu_util']:>5}%)")

    low_util = [r for r in results if r["score"] < THRESHOLD]
    if low_util:
        print(f"\nâš ï¸ æ£€æµ‹åˆ° {len(low_util)} å¼  GPU åˆ©ç”¨ç‡ä½äº {THRESHOLD}%ï¼š")
        for r in low_util:
            print(f"  - {r['namespace']}/{r['pod']} ({r['score']}%)")
            try:
                # å¯é€‰ï¼šè°ƒç”¨æ§åˆ¶API
                requests.post(CONTROL_API, json={
                    "gpu_id": r["gpu_id"],
                    "pod": r["pod"],
                    "namespace": r["namespace"],
                    "score": r["score"]
                }, timeout=5)
            except Exception as e:
                print(f"[WARN] è°ƒç”¨æ§åˆ¶æ¥å£å¤±è´¥: {e}")


@app.on_event("startup")
async def startup_event():
    """åå°å‘¨æœŸä»»åŠ¡"""
    async def scheduler():
        while True:
            await analyze_all_gpus()
            await asyncio.sleep(INTERVAL)

    asyncio.create_task(scheduler())


@app.get("/analyze")
async def trigger_analysis(bg: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡ GPU åˆ©ç”¨ç‡åˆ†æ"""
    bg.add_task(analyze_all_gpus)
    return {"status": "started"}


@app.get("/")
async def root():
    return {"service": "GPU Utilization Monitor", "interval": INTERVAL, "threshold": THRESHOLD}
```

---

## ğŸ§© éƒ¨ç½²æ–¹å¼

### ğŸ³ 1. ç›´æ¥è¿è¡Œ
```bash
pip install fastapi uvicorn numpy requests
uvicorn gpu_monitor:app --host 0.0.0.0 --port 8080
```

### ğŸ§­ 2. è®¿é—®æ¥å£
- æ‰‹åŠ¨è§¦å‘åˆ†æï¼š`GET /analyze`
- æŸ¥çœ‹é…ç½®ï¼š`GET /`
- è‡ªåŠ¨æ¯ 5 åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡è½®è¯¢

### ğŸ› ï¸ 3. è¾“å‡ºæ—¥å¿—ç¤ºä¾‹
```
========== GPU åˆ©ç”¨ç‡åˆ†æ ==========
[team-a/ns-train1] GPU GPU-a123 â†’ Score=  85% (Util= 78%)
[team-b/ns-lab3]  GPU GPU-b789 â†’ Score=   5% (Util=  2%)

âš ï¸ æ£€æµ‹åˆ° 1 å¼  GPU åˆ©ç”¨ç‡ä½äº 20%ï¼š
  - team-b/ns-lab3 (5%)
```

---

## ğŸŒŸ å¯é€‰æ‰©å±•å»ºè®®

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| ğŸ§  **AI æ¨¡å‹åˆ¤æ–­** | å°†æ¯å¼  GPU æœ€è¿‘10åˆ†é’ŸæŒ‡æ ‡åºåˆ—ä½œä¸ºç‰¹å¾è¾“å…¥åˆ° LLMï¼Œæ™ºèƒ½åˆ¤æ–­æ˜¯å¦ç©ºé—² |
| ğŸ’¾ **ä¿å­˜å†å²æ•°æ®** | æŠŠæ¯è½®åˆ†æç»“æœå†™å…¥ SQLite / InfluxDBï¼Œç”¨äºè¶‹åŠ¿å¯è§†åŒ– |
| ğŸ•¹ **Grafana è§†å›¾** | æ˜¾ç¤ºæ¯å¼  GPU åˆ©ç”¨ç‡è¯„åˆ†ã€pod/namespaceã€è¶‹åŠ¿å›¾ |
| ğŸ”” **å‘Šè­¦é€šçŸ¥** | æ¥å…¥ Slack / é£ä¹¦ / é‚®ä»¶ é€šçŸ¥ä½åˆ©ç”¨ GPU |
| ğŸ§© **ä¸ Kubeflow é›†æˆ** | è°ƒç”¨ Kubeflow Notebook / Job API è‡ªåŠ¨ç¼©å®¹æˆ–åœæ­¢ç©ºé—²å®ä¾‹ |

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ï¼š
1ï¸âƒ£ åœ¨è¿™ä¸ªæœåŠ¡ä¸Šå†åŠ ä¸Š **LLM åˆ¤æ–­é€»è¾‘ï¼ˆæ¯”å¦‚åˆ©ç”¨AIåˆ†æ10åˆ†é’Ÿåºåˆ—æ˜¯å¦åœ¨è®­ç»ƒï¼‰**ï¼Œ  
è¿˜æ˜¯  
2ï¸âƒ£ å¸®ä½ åŠ ä¸Š **ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“ + Grafanaå¯è§†åŒ–æ¥å£**ï¼Ÿ
user
DCGM_FI_DEV_POWER_USAGE{DCGM_FI_DRIVER_VERSION="545.23.08", Hostname="aigc-k8s-vn-master001", UUID="GPU-03cd1cf6-11f6-5ecc-feb0-af07406e1838", alert_group="aigc", app_kubernetes_io_component="dcgm-exporter", app_kubernetes_io_instance="nvidia-gpu", app_kubernetes_io_name="dcgm-exporter", container="comfyui", controller_revision_hash="5b87bdbcd9", device="nvidia0", gpu="0", group="vm_aigc", idc="aigc-vn", instance="10.244.0.50:9400", job="lens-metrics/nvidia-gpu-dcgm-exporter", k8s_cluster="aigc-k8s-vn", kubernetes_namespace="comfyui", kubernetes_node="aigc-k8s-vn-master001", kubernetes_pod_name="nvidia-gpu-dcgm-exporter-d96xw", labrador_project="AIGC", modelName="Tesla V100S-PCIE-32GB", namespace="comfyui", pod="comfyui-xuanwu-deploy-694d64fb5-jqs8w", pod_template_generation="1", target_endpoint="pods", zone_name="vn"}

è¿™æ˜¯æŸ¥è¯¢å“åº”å†…å®¹ï¼Œä½ å¯ä»¥å‚è€ƒä¸€ä¸‹
ChatGPT
éå¸¸å¥½ï¼Œä½ è¿™æ¡ Prometheus å“åº”æ ·ä¾‹ä¿¡æ¯éå¸¸å…³é”®ï¼Œå®ƒè¯´æ˜ä½ çš„æ•°æ®ç»“æ„å·²ç»æ ‡å‡†åŒ–äº†ï¼Œå¯ä»¥é€šè¿‡ labelï¼ˆå¦‚ `gpu`ã€`pod`ã€`namespace`ã€`Hostname`ï¼‰åŒºåˆ†ä¸åŒ GPU å®ä¾‹ï¼Œéå¸¸é€‚åˆåšè‡ªåŠ¨åŒ–å¤š GPU åˆ©ç”¨ç‡åˆ†æã€‚  

---

## âœ… ä¸€ã€ç›®æ ‡
ä½ çš„ç›®æ ‡æ˜¯ï¼š
1. å¯¹æ¯ä¸ª GPUï¼ˆå¯é€šè¿‡ `UUID` æˆ– `gpu` label å”¯ä¸€è¯†åˆ«ï¼‰è®¡ç®—å‡ºä¸€ä¸ª **ç»¼åˆåˆ©ç”¨ç‡è¯„åˆ†**ï¼›
2. æ”¯æŒå¤šçº¿ç¨‹ï¼ˆæˆ–å¼‚æ­¥ï¼‰å¹¶è¡Œåˆ†æï¼›
3. å½“åˆ©ç”¨ç‡ä½äºé˜ˆå€¼æ—¶è‡ªåŠ¨è°ƒç”¨æ¥å£ï¼ˆæ¯”å¦‚é‡Šæ”¾ GPUã€æé†’ç§Ÿæˆ·ã€æˆ–ç¼©å®¹ï¼‰ï¼›
4. æŸ¥è¯¢ Prometheus æ—¶åŠ ä¸Š `{pod!=""}` è¿‡æ»¤æ¡ä»¶ã€‚

---

## âœ… äºŒã€æ¨èçš„ç›‘æ§æ–¹æ¡ˆæ¶æ„
æ•´ä½“æµç¨‹å›¾å¦‚ä¸‹ï¼š

```
[Prometheus] 
   â†“ (promQL æŸ¥è¯¢ GPU metrics)
[GPU Monitor Service]
   â†“
[AI/Rule-based Analyzer] (è®¡ç®—ç»¼åˆåˆ©ç”¨ç‡è¯„åˆ†)
   â†“
[Decision Engine]
   â†“
[Action API è°ƒç”¨ (é‡Šæ”¾/æé†’/ç¼©å®¹)]
```

ä½ å¯ä»¥å®ç°æˆä¸€ä¸ª Python æœåŠ¡ï¼Œå®šæ—¶æ‹‰å–æŒ‡æ ‡ã€‚

---

## âœ… ä¸‰ã€Prometheus æŸ¥è¯¢æŒ‡æ ‡å»ºè®®

ä¸ºäº†è®© AI æˆ–ç®—æ³•åˆç†åˆ¤æ–­ GPU æ˜¯å¦â€œçœŸçš„åœ¨ç”¨â€ï¼Œå»ºè®®åŒæ—¶é‡‡é›†ä»¥ä¸‹ metricsï¼š

| æŒ‡æ ‡å | è¯´æ˜ | æ˜¯å¦å…³é”® |
|--------|------|-----------|
| `DCGM_FI_DEV_GPU_UTIL` | GPU æ ¸å¿ƒåˆ©ç”¨ç‡ (%) | âœ… é«˜ |
| `DCGM_FI_DEV_FB_USED` | æ˜¾å­˜ä½¿ç”¨é‡ (MiB) | âœ… ä¸­ |
| `DCGM_FI_DEV_FB_TOTAL` | æ˜¾å­˜æ€»é‡ (MiB) | âš ï¸ï¼ˆä½ æ²¡æ•°æ®ï¼Œå¯å¿½ç•¥æˆ–ä»å…¶ä»– metric æ¨å¯¼ï¼‰ |
| `DCGM_FI_DEV_SM_CLOCK` | æ ¸å¿ƒé¢‘ç‡ (MHz) | âœ… è¾…åŠ©åˆ¤æ–­æ´»è·ƒåº¦ |
| `DCGM_FI_DEV_MEM_CLOCK` | æ˜¾å­˜é¢‘ç‡ (MHz) | âœ… è¾…åŠ© |
| `DCGM_FI_DEV_POWER_USAGE` | å®é™…åŠŸç‡ (W) | âœ… éå¸¸å…³é”®ï¼ˆå¯ä»¥åˆ¤æ–­ GPU æ˜¯å¦å¤„äºä½åŠŸè€—é—²ç½®çŠ¶æ€ï¼‰ |
| `DCGM_FI_PROF_PIPE_TENSOR_ACTIVE` | Tensor Core åˆ©ç”¨ç‡ | âœ… å¯é€‰ï¼Œç”¨äºæ·±åº¦å­¦ä¹ åœºæ™¯ |
| `DCGM_FI_PROF_GR_ENGINE_ACTIVE` | å›¾å½¢å¼•æ“åˆ©ç”¨ç‡ | âœ… å¯é€‰ |
| `DCGM_FI_DEV_TEMP` | æ¸©åº¦ | âš™ï¸ è¾…åŠ©æŒ‡æ ‡ |

---

## âœ… å››ã€ç»¼åˆåˆ©ç”¨ç‡è¯„åˆ†è®¡ç®—æ–¹æ¡ˆ

å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ª **AIå‰ç½®å¯å‘å¼è¯„åˆ†å‡½æ•°**ï¼š

```python
def gpu_utilization_score(metrics):
    """
    è®¡ç®—GPUç»¼åˆåˆ©ç”¨ç‡è¯„åˆ† (0~100)
    """
    gpu_util = metrics.get("DCGM_FI_DEV_GPU_UTIL", 0)
    fb_used = metrics.get("DCGM_FI_DEV_FB_USED", 0)
    sm_clock = metrics.get("DCGM_FI_DEV_SM_CLOCK", 0)
    mem_clock = metrics.get("DCGM_FI_DEV_MEM_CLOCK", 0)
    power_usage = metrics.get("DCGM_FI_DEV_POWER_USAGE", 0)

    # ç®€å•å½’ä¸€åŒ–é€»è¾‘ï¼Œå¯è¢«AIæ¨¡å‹æ›¿æ¢
    score = (
        0.5 * gpu_util +
        0.2 * (fb_used / 32000 * 100) +  # å‡è®¾V100S 32GB
        0.15 * (power_usage / 250 * 100) +
        0.1 * (sm_clock / 1600 * 100) +
        0.05 * (mem_clock / 1200 * 100)
    )
    return round(score, 2)
```

å¦‚æœè¯„åˆ†ä½äºæŸä¸ªé˜ˆå€¼ï¼ˆä¾‹å¦‚ < 10ï¼‰ï¼Œä¸”æŒç»­æ—¶é—´è¶…è¿‡ 10 åˆ†é’Ÿï¼Œåˆ™è®¤ä¸ºâ€œç©ºé—²â€ã€‚

---

## âœ… äº”ã€Prometheus æŸ¥è¯¢æ ·ä¾‹

ä»¥ `aiohttp` + `prometheus_api_client` æ–¹å¼å®ç°å¼‚æ­¥å¤š GPU æ‹‰å–ï¼š

```python
import asyncio
from prometheus_api_client import PrometheusConnect
from concurrent.futures import ThreadPoolExecutor

prom = PrometheusConnect(url="http://prometheus.yourdomain:9090", disable_ssl=True)

QUERY_METRICS = [
    "DCGM_FI_DEV_GPU_UTIL",
    "DCGM_FI_DEV_FB_USED",
    "DCGM_FI_DEV_SM_CLOCK",
    "DCGM_FI_DEV_MEM_CLOCK",
    "DCGM_FI_DEV_POWER_USAGE",
]

def query_metric(metric):
    query = f'{metric}{{pod!=""}}'
    return prom.custom_query(query=query)

def analyze_gpu(gpu_data):
    # æ±‡æ€»æ¯ä¸ª GPU çš„æŒ‡æ ‡å€¼
    metrics = {}
    for d in gpu_data:
        gpu_id = d['metric'].get('UUID') or d['metric'].get('gpu')
        value = float(d['value'][1])
        metric_name = d['metric']['__name__']
        metrics.setdefault(gpu_id, {})[metric_name] = value

    results = {}
    for gpu_id, vals in metrics.items():
        results[gpu_id] = gpu_utilization_score(vals)
    return results

async def main():
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor() as pool:
        gpu_data_all = await asyncio.gather(*[
            loop.run_in_executor(pool, query_metric, m) for m in QUERY_METRICS
        ])
        gpu_data_flat = [item for sublist in gpu_data_all for item in sublist]
        results = analyze_gpu(gpu_data_flat)
        print(results)

asyncio.run(main())
```

è¾“å‡ºç¤ºä¾‹ï¼š
```bash
{
  "GPU-03cd1cf6-11f6-5ecc-feb0-af07406e1838": 12.4,
  "GPU-a24b...": 83.7
}
```

---

## âœ… å…­ã€æ‰©å±•ï¼šè°ƒç”¨AIè¾…åŠ©åˆ¤æ–­

ä½ å¯ä»¥åœ¨åˆ†æé˜¶æ®µå°†æœ€è¿‘5åˆ†é’Ÿçš„æ—¶åºæ•°æ®ï¼ˆå¦‚åŠŸè€—ã€æ˜¾å­˜ã€GPU_UTILæ›²çº¿ï¼‰è¾“å…¥AIæ¨¡å‹ï¼Œè®©AIè¾“å‡ºåˆ¤æ–­ï¼Œä¾‹å¦‚ï¼š

```python
prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹GPUæŒ‡æ ‡æ•°æ®ï¼Œåˆ¤æ–­è¯¥GPUæ˜¯å¦æ­£åœ¨è¢«æœ‰æ•ˆåˆ©ç”¨ã€‚
æ•°æ®: {json.dumps(gpu_timeseries, ensure_ascii=False)}
è¿”å›ä¸€ä¸ª0~100çš„åˆ©ç”¨ç‡è¯„åˆ†ã€‚
"""
```

è¿™æ ·AIä¼šç»¼åˆæ—¶é—´åºåˆ—å˜åŒ–è¶‹åŠ¿ï¼ˆæ¯”å¦‚GPU_UTILæ³¢åŠ¨ã€åŠŸè€—ä¸Šå‡ã€æ˜¾å­˜å ç”¨ç¨³å®šç­‰ï¼‰æ¥åˆ¤æ–­æ˜¯å¦çœŸçš„åœ¨è·‘ä»»åŠ¡ã€‚

---

## âœ… ä¸ƒã€åç»­æ‰©å±•å»ºè®®
- âœ… å°†ç»“æœå†™å…¥ Prometheus PushGateway æˆ– Grafana Lokiï¼Œå½¢æˆâ€œAIåˆ©ç”¨ç‡â€æŒ‡æ ‡ï¼›
- âœ… å¢åŠ ä¸€ä¸ªæŒç»­æ€§åˆ¤å®šé€»è¾‘ï¼Œæ¯”å¦‚è¿ç»­ N æ¬¡é‡‡æ ·è¯„åˆ†ä½æ‰è§¦å‘å›æ”¶ï¼›
- âœ… ç»“åˆ K8s API è‡ªåŠ¨ labelã€é©±åŠ¨ Pod é©±é€æˆ–é€šçŸ¥ã€‚

---

æˆ‘å¯ä»¥å¸®ä½ æŠŠä¸Šé¢çš„æ–¹æ¡ˆæ‰©å±•æˆä¸€ä¸ªå®Œæ•´çš„ Python æœåŠ¡ï¼ˆå¸¦ Prometheus æ‹‰å–ã€AIåˆ†æã€REST API è°ƒç”¨ã€å¼‚æ­¥è°ƒåº¦æ¨¡å—ï¼‰ã€‚  
æ˜¯å¦å¸Œæœ›æˆ‘ç›´æ¥å¸®ä½ ç”Ÿæˆè¿™ä¸ªå®Œæ•´æ¡†æ¶ï¼Ÿ